{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22a0291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/rb666/Projects/MOSAIC/EVAL/dreamachine/stability_tests\n",
      "project_root: /Users/rb666/Projects/MOSAIC\n",
      "configs exists: True\n",
      "Transformer model: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from scipy.stats import pearsonr\n",
    "# import itertools\n",
    "import os,sys \n",
    "current_dir = os.getcwd()\n",
    "# project_root = os.path.dirname(current_dir)  \n",
    "project_root = \"/Users/rb666/Projects/MOSAIC\" #/raid/home/rbeaute/Projects/MOSAIC\"\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "print(f\"cwd: {os.getcwd()}\")\n",
    "print(f\"project_root: {project_root}\")\n",
    "print(f\"configs exists: {os.path.exists(os.path.join(project_root, 'configs'))}\")\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from configs.dreamachine2 import config\n",
    "# from src.preprocessor import preproc\n",
    "from preproc.preprocessor import preproc\n",
    "\n",
    "EMBEDDING_MODEL_NAME = config.transformer_model\n",
    "print(f\"Transformer model: {EMBEDDING_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce89d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/rb666/Projects/MOSAIC/EVAL/dreamachine/stability_tests\n",
      "Retrieving data from BOX, locally stored at: /Users/rb666/Library/CloudStorage/Box-Box/TMDATA\n",
      "Data directory: /Users/rb666/Library/CloudStorage/Box-Box/TMDATA/DREAMACHINE\n",
      "Using data from: /Users/rb666/Library/CloudStorage/Box-Box/TMDATA/DREAMACHINE/DL_reflections_APIcleaned.csv\n",
      "\n",
      "Successfully loaded and processed 205 sentences.\n",
      "After removing short sentences, 198 sentences remain.\n",
      "After removing duplicates, 198 remain.\n",
      "Loaded and preprocessed 198 sentences for condition 'DL'.\n"
     ]
    }
   ],
   "source": [
    "dataset = \"DREAMACHINE\"\n",
    "condition = \"DL\"  # \"HS\" or \"DL\"\n",
    "sentences = True\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "BOX_DIR = os.path.join(os.path.expanduser(\"~\"), \"Library\", \"CloudStorage\", \"Box-Box\", \"TMDATA\")\n",
    "print(f\"Retrieving data from BOX, locally stored at: {BOX_DIR}\")\n",
    "DATA_DIR = os.path.join(BOX_DIR, dataset)\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "results_dir = os.path.join(project_root, \"EVAL\",dataset.lower())\n",
    "\n",
    "reports_path = os.path.join(DATA_DIR, f\"{condition}_reflections_APIcleaned.csv\")\n",
    "print(\"Using data from:\", reports_path)\n",
    "\n",
    "# load data and divide into sentences if needed\n",
    "df_reports = pd.read_csv(reports_path)['cleaned_reflection'].dropna().reset_index(drop=True)\n",
    "df_reports\n",
    "\n",
    "#preproc reports using preproc function in src\n",
    "df_reports = preproc(df_reports)\n",
    "print(f\"Loaded and preprocessed {len(df_reports)} sentences for condition '{condition}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e571ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MANUAL optimal parameters for Deep Listening (DL)...\n",
      "Parameters loaded:\n",
      "{'n_components': 7, 'n_neighbors': 8, 'min_dist': 0.04, 'min_cluster_size': 10, 'min_samples': 9}\n"
     ]
    }
   ],
   "source": [
    "# --- PARAMETER SELECTION (from optuna results) ---\n",
    "\n",
    "if condition == \"DL\":\n",
    "    print(\"Using MANUAL optimal parameters for Deep Listening (DL)...\")\n",
    "    chosen_params = {\n",
    "        'n_components': 7,\n",
    "        'n_neighbors': 8,\n",
    "        'min_dist': 0.04,\n",
    "        'min_cluster_size': 10,\n",
    "        'min_samples': 9\n",
    "    }\n",
    "elif condition == \"HS\":\n",
    "    print(\"Using MANUAL optimal parameters for High Sensory (HS)...\")\n",
    "    chosen_params = {\n",
    "        'n_components': 20,\n",
    "        'n_neighbors': 26,\n",
    "        'min_dist': 0.015,\n",
    "        'min_cluster_size': 10,\n",
    "        'min_samples': 8\n",
    "    }\n",
    "\n",
    "print(\"Parameters loaded:\")\n",
    "print(chosen_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808aa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "NUM_BOOTSTRAPS = 100    # As requested by reviewer\n",
    "SAMPLE_FRAC = 0.80      # 80% subsample\n",
    "top_n_words = 15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. SETUP MODELS (Fixed Parameters) ---\n",
    "# We use the exact parameters found by Optuna (chosen_params)\n",
    "# This ensures we test DATA stability, not parameter stability.\n",
    "\n",
    "embedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\") \n",
    "\n",
    "def create_model_instance():\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=int(chosen_params['n_neighbors']),\n",
    "        n_components=int(chosen_params['n_components']),\n",
    "        min_dist=float(chosen_params['min_dist']),\n",
    "        metric='cosine',\n",
    "        random_state=42  # Fixed seed for UMAP to isolate data variance\n",
    "    )\n",
    "    \n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=int(chosen_params['min_cluster_size']),\n",
    "        min_samples=int(chosen_params['min_samples']),\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\") \n",
    "\n",
    "    return BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        top_n_words=top_n_words,\n",
    "        nr_topics=\"auto\",\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fa8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Reference Model on 100% of data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f876951f059c41829f76b2960f815dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Model has 7 topics.\n",
      "\n",
      "Starting 100 Bootstrap Iterations (80.0% sample)...\n",
      "Completed 10/100 runs...\n",
      "Completed 20/100 runs...\n",
      "Completed 30/100 runs...\n",
      "Completed 40/100 runs...\n",
      "Completed 50/100 runs...\n",
      "Completed 60/100 runs...\n",
      "Completed 70/100 runs...\n",
      "Completed 80/100 runs...\n",
      "Completed 90/100 runs...\n",
      "Completed 100/100 runs...\n",
      "\n",
      "--- BOOTSTRAP ROBUSTNESS RESULTS ---\n",
      "Topic 0: Mean Sim = 0.969 (+/- 0.023)\n",
      "Topic 1: Mean Sim = 0.956 (+/- 0.020)\n",
      "Topic 2: Mean Sim = 0.980 (+/- 0.025)\n",
      "Topic 3: Mean Sim = 0.894 (+/- 0.060)\n",
      "Topic 4: Mean Sim = 0.911 (+/- 0.039)\n",
      "Topic 5: Mean Sim = 0.900 (+/- 0.048)\n",
      "Topic 6: Mean Sim = 0.906 (+/- 0.032)\n",
      "\n",
      "GLOBAL STABILITY (Average Cosine Similarity): 0.931 (+/- 0.033)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 2. TRAIN REFERENCE MODEL (100% Data) ---\n",
    "print(\"Training Reference Model on 100% of data...\")\n",
    "sentences_array = df_reports\n",
    "embeddings_ref = embedding_model.encode(sentences_array, show_progress_bar=True)\n",
    "\n",
    "ref_model = create_model_instance()\n",
    "topics_ref, probs_ref = ref_model.fit_transform(sentences_array, embeddings_ref)\n",
    "\n",
    "# Get Reference Centroids (Topic Embeddings)\n",
    "# Note: topic_embeddings_ is a list where index corresponds to topic ID + 1 (usually)\n",
    "# Use get_topic_info() to map ID to actual embedding safely\n",
    "ref_info = ref_model.get_topic_info()\n",
    "ref_topics = ref_info[ref_info['Topic'] != -1]['Topic'].tolist() # Exclude outliers\n",
    "\n",
    "# Extract actual embeddings for valid topics\n",
    "ref_centroids = []\n",
    "for tid in ref_topics:\n",
    "    #  retrieves the embedding vector for topic ID 'tid'\n",
    "    ref_centroids.append(ref_model.topic_embeddings_[tid + ref_model._outliers]) \n",
    "\n",
    "ref_centroids = np.array(ref_centroids)\n",
    "print(f\"Reference Model has {len(ref_topics)} topics.\")\n",
    "\n",
    "\n",
    "# --- 3. BOOTSTRAP LOOP ---\n",
    "print(f\"\\nStarting {NUM_BOOTSTRAPS} Bootstrap Iterations ({SAMPLE_FRAC*100}% sample)...\")\n",
    "\n",
    "similarity_scores = {tid: [] for tid in ref_topics} # Store scores for each topic separately\n",
    "\n",
    "for i in range(NUM_BOOTSTRAPS):\n",
    "    #  sample indices so we can grab the corresponding pre-computed embeddings (faster)\n",
    "    n_samples = int(len(sentences_array) * SAMPLE_FRAC)\n",
    "    indices = np.random.choice(len(sentences_array), n_samples, replace=False)\n",
    "    \n",
    "    subset_sentences = [sentences_array[j] for j in indices]\n",
    "    subset_embeddings = embeddings_ref[indices]\n",
    "    \n",
    "    # train new model on subset\n",
    "    bs_model = create_model_instance()\n",
    "    bs_model.fit(subset_sentences, subset_embeddings)\n",
    "    \n",
    "    # extract new centroids\n",
    "    bs_info = bs_model.get_topic_info()\n",
    "    bs_valid_topics = bs_info[bs_info['Topic'] != -1]['Topic'].tolist()\n",
    "    \n",
    "    if not bs_valid_topics:\n",
    "        print(f\"Iter {i}: No topics found. Skipping.\")\n",
    "        continue\n",
    "        \n",
    "    bs_centroids = np.array([bs_model.topic_embeddings_[tid + bs_model._outliers] for tid in bs_valid_topics])\n",
    "    \n",
    "    # compare: find best match for each reference topic\n",
    "    # calculate similarity matrix (Rows: Ref Topics, Cols: Bootstrap Topics)\n",
    "    sim_matrix = cosine_similarity(ref_centroids, bs_centroids)\n",
    "    \n",
    "    # for each reference topic, find the max similarity in the new model\n",
    "    for idx, topic_id in enumerate(ref_topics):\n",
    "        best_match_score = np.max(sim_matrix[idx]) # Best match found in this bootstrap run\n",
    "        similarity_scores[topic_id].append(best_match_score)\n",
    "        \n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Completed {i+1}/{NUM_BOOTSTRAPS} runs...\")\n",
    "\n",
    "# --- 4. RESULTS AGGREGATION ---\n",
    "print(\"\\n--- BOOTSTRAP ROBUSTNESS RESULTS ---\")\n",
    "final_scores = []\n",
    "for tid in ref_topics:\n",
    "    scores = similarity_scores[tid]\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    final_scores.append(mean_score)\n",
    "    print(f\"Topic {tid}: Mean Sim = {mean_score:.3f} (+/- {std_score:.3f})\")\n",
    "\n",
    "grand_mean = np.mean(final_scores)\n",
    "grand_std = np.std(final_scores)\n",
    "\n",
    "print(f\"\\nGLOBAL STABILITY (Average Cosine Similarity): {grand_mean:.3f} (+/- {grand_std:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mosaicvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
